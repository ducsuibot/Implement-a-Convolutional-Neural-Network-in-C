ğŸ§  CNN Implementation in C from Scratch
ğŸ“Œ Introduction
This project implements a Convolutional Neural Network (CNN) from the ground up using the C programming language, without relying on pre-built deep learning libraries like TensorFlow or PyTorch. The primary goals are:

Gain a deep understanding of CNN operations at a low level.
Build the entire pipeline, from forward propagation to classification, manually.
Experiment with the handwritten digit recognition task using the MNIST dataset (28x28 grayscale images, 10 classes).

âš™ï¸ Model Architecture
The implemented CNN follows the architecture depicted below:

The layers are:

Input Layer: 28Ã—28Ã—1 input image.
Convolution Layer 1: 5Ã—5 kernel, stride 1, output 24Ã—24Ã—2 â†’ ReLU activation.
MaxPooling Layer 1: 2Ã—2 kernel â†’ output 12Ã—12Ã—2.
Convolution Layer 2: 3Ã—3 kernel, 4 filters â†’ output 10Ã—10Ã—4 â†’ Sigmoid activation.
MaxPooling Layer 2: 2Ã—2 kernel â†’ output 5Ã—5Ã—4.
Flatten Layer: Converts 5Ã—5Ã—4 tensor to a 100-dimensional vector.
Fully Connected Layer:
100 nodes â†’ 10 nodes.
Activation function: Softmax.


Output: Predicts digits from 0â€“9.

ğŸ“‚ Directory Structure
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cnn.c          # CNN implementation
â”‚   â”œâ”€â”€ layers.c       # Layer implementations: conv, pooling, fc
â”‚   â”œâ”€â”€ utils.c        # Utility functions: data loading, weight initialization
â”‚   â””â”€â”€ main.c         # Main program
â”œâ”€â”€ data/
â”‚   â””â”€â”€ mnist/         # MNIST dataset (.idx format)
â”œâ”€â”€ include/
â”‚   â””â”€â”€ *.h            # Header files
â””â”€â”€ README.md

ğŸš€ How to Run

Clone the repository:

git clone https://github.com/<username>/cnn-from-scratch-c.git
cd cnn-from-scratch-c


Compile the code:

gcc src/*.c -o cnn -lm


Execute the program:

./cnn


Expected Output:
Displays loss and accuracy per epoch.
Prints predictions for a few MNIST images.



ğŸ“Š Training Results
The model was trained on the MNIST dataset with the following results:
Loading MNIST from E:/CNN/train-images.idx3-ubyte, E:/CNN/train-labels.idx1-ubyte, E:/CNN/t10k-images.idx3-ubyte, E:/CNN/t10k-labels.idx1-ubyte ...
Loaded.
Epoch 1 - Loss: 0.3812 - Accuracy: 88.58%
  -> Test - Loss: 0.1728 - Accuracy: 94.72%
Epoch 2 - Loss: 0.1506 - Accuracy: 95.60%
  -> Test - Loss: 0.1400 - Accuracy: 95.62%
Epoch 3 - Loss: 0.1281 - Accuracy: 96.24%
  -> Test - Loss: 0.1253 - Accuracy: 96.06%
Epoch 4 - Loss: 0.1170 - Accuracy: 96.56%
  -> Test - Loss: 0.1242 - Accuracy: 96.18%
Epoch 5 - Loss: 0.1101 - Accuracy: 96.65%
  -> Test - Loss: 0.1170 - Accuracy: 96.37%


Achieves a test accuracy of up to 96.37% after 5 epochs.
Training time is longer compared to Python/Frameworks due to the absence of GPU optimization.

ğŸ“š Applied Knowledge

Core concepts: Convolution, Pooling, Flatten, Fully Connected, Softmax.
Forward propagation mechanics.
Loss function: Cross-Entropy.
Optimization technique: Stochastic Gradient Descent (SGD).
